# ======================================================
# ë³¸ê²© í¸ì§‘ ì¤‘ REV7 - PandasAI Streamlit App (CTk ë¡œì§ ì™„ì „ ì´ì‹)
# ======================================================

import streamlit as st
import pandas as pd
from pandasai import SmartDataframe
from pandasai.llm.openai import OpenAI
import openai
from typing import Optional, Any, Dict, List, Tuple
import re
import sys
import json
import os

# ======================================================
# 0. ì„¤ì • ë° ìƒìˆ˜ ì •ì˜
# ======================================================
LLM_MODEL = "gpt-3.5-turbo"  # "gpt-3.5-turbo", "gpt-4o"
RESET_ON_QUERY = True  # True: ë§¤ ì¿¼ë¦¬ë§ˆë‹¤ SmartDataframe ì¬ìƒì„± / False: ì„¸ì…˜ ì¬ì‚¬ìš©

# ======================================================
# ğŸ“Œ LLM ë™ì‘ ê·œì¹™ (ì›ë³¸ ê·¸ëŒ€ë¡œ)
# ======================================================
CUSTOM_INSTRUCTION = """
ì´ ë°ì´í„°í”„ë ˆì„ì˜ ë¶„ì„ì„ ìœ„í•´ ë°˜ë“œì‹œ ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¥´ì„¸ìš”.

========================================================
1. DataFrame ì‚¬ìš© ê·œì¹™
========================================================
- SmartDataframe ë‚´ë¶€ì˜ df 'í•˜ë‚˜ë§Œ' ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
- df ì™¸ì— dfs, temp_df, new_df ë“± ìƒˆë¡œìš´ ë¦¬ìŠ¤íŠ¸ë‚˜ ë°ì´í„°í”„ë ˆì„ì„ ë§Œë“¤ì§€ ë§ˆì‹­ì‹œì˜¤.
- ì ˆëŒ€ dfë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ê±°ë‚˜ ë°˜ë³µë¬¸ìœ¼ë¡œ ì²˜ë¦¬í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.

========================================================
2. í•„í„°ë§ ê·œì¹™ (í•µì‹¬)
========================================================
DataFrame í•„í„°ë§ì€ ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ë§Œ í—ˆìš©í•©ë‹ˆë‹¤:

df_filtered = df[
    (df['ì»¬ëŸ¼'] == ê°’) &
    (df['ì»¬ëŸ¼'] == ê°’)
]

ì•„ë˜ ë™ì‘ì€ ì ˆëŒ€ ê¸ˆì§€í•©ë‹ˆë‹¤:
- (df['ì»¬ëŸ¼'] == ê°’).all()
- for df in dfs
- dfs = [df for df in dfs ...]
- pd.concat()
- ì—¬ëŸ¬ ê°œì˜ dfë¥¼ ë¦¬ìŠ¤íŠ¸ì— ë‹´ì•„ ì²˜ë¦¬

========================================================
3. ê·¸ë£¹ë°”ì´/ì§‘ê³„ ê·œì¹™
========================================================
- ì§‘ê³„(sum, mean ë“±)ëŠ” ë‹¨ì¼ df ê°ì²´ì—ì„œë§Œ ìˆ˜í–‰í•˜ì‹­ì‹œì˜¤.
- df.groupby(...) ëŠ” í—ˆìš©ë©ë‹ˆë‹¤.
- df_list, concat, merge ë“± ë‘ ê°œ ì´ìƒì˜ DFë¥¼ ë§Œë“¤ì–´ ì¡°ì‘í•˜ëŠ” í–‰ìœ„ë¥¼ ê¸ˆì§€í•©ë‹ˆë‹¤.

========================================================
4. ê²°ê³¼ ë°˜í™˜ ê·œì¹™
========================================================
ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤:

result = {"type": "dataframe", "value": df_filtered}

========================================================
5. ì½”ë“œ ì•ˆì „ ê·œì¹™
========================================================
- Python ë¬¸ë²• ì˜¤ë¥˜ê°€ ë°œìƒí•˜ëŠ” ì½”ë“œëŠ” ìƒì„±í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
- ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë³€ìˆ˜(dfs, temp_df ë“±)ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
"""

# ======================================================
# ì»¬ëŸ¼ ë° ê°’ ë™ì˜ì–´ (ì›ë³¸ ê·¸ëŒ€ë¡œ)
# ======================================================
COLUMN_SYNONYMS = {
    "ì¥ë¹„ëª…": ["ì¥ë¹„"],
    "UT": ["ê³µì¢…", "ì„¤ë¹„", "ìœ í‹¸ë¦¬í‹°", "utility"],
    "Floor": ["ì¸µìˆ˜", "í”Œë¡œì–´"],
    "ì‚¬ì „ì œì‘X_ë¹„ëŒ€ìƒ(ì¼ë¶€ê³µì •)(1)_ê¸¸ì´": ["ë¹„ëŒ€ìƒ"],
    "ì‚¬ì „ì œì‘X_A(ì¥ë¹„ë‹¨Final)(2)_ê¸¸ì´": ["ì¥ë¹„ë‹¨"],
    "ì‚¬ì „ì œì‘â—‹_B(H_UPêµ¬ê°„)(3)_ë‹¹ì´ˆê³„íš_ê¸¸ì´": ["ê³„íšë¬¼ëŸ‰"],
    "ì‚¬ì „ì œì‘â—‹_B(H_UPêµ¬ê°„)(4)_ì‹¤ì œì‹œê³µ_ê¸¸ì´": ["ì‹œê³µë¬¼ëŸ‰"],
    "ì‚¬ì „ì œì‘X_C(TVë‹¨Final)(5)_ê¸¸ì´": ["í…Œí•‘ë°¸ë¸Œë‹¨"],
    "í•©ê³„(1+2+4+5)": ["ì´í•©"]
}

VALUE_SYNONYMS = {
    "1F": ["1ì¸µ"],
    "2F": ["2ì¸µ"],
    "3F": ["3ì¸µ"],
    "Bulk Gas": ["ë²Œí¬ê°€ìŠ¤", "bulk gas"],
    "Drain": ["ë“œë ˆì¸", "drain"],
    "Exhaust": ["ì´ê·¸ì €ìŠ¤íŠ¸", "exhaust"],
    "UPW(DI)": ["ì´ˆìˆœìˆ˜"],
    "PCW": ["í”„ë¡œì„¸ìŠ¤ì¿¨ë§ì›Œí„°"],
    "NPW": ["ê³µì—…ìš©ìˆ˜"],
    "Chemical": ["ì¼€ë¯¸ì¹¼", "chemical"],
    "Pumping": ["íŒí”„", "pumping"],
    "Toxic Gas": ["í†¡ì‹ê°€ìŠ¤", "toxic gas"],
}

# ======================================================
# Streamlit í˜ì´ì§€ ì„¤ì •
# ======================================================
st.set_page_config(
    page_title="ğŸ“Š PandasAI ëŒ€í™”í˜• ë°ì´í„° ë¶„ì„ê¸° (Streamlit)",
    layout="wide"
)

st.sidebar.subheader("ğŸ” OpenAI API í‚¤ ì…ë ¥")
if "OPENAI_API_KEY" not in st.session_state:
    st.session_state.OPENAI_API_KEY = ""


api_key_input = st.sidebar.text_input("OpenAI API Key ì…ë ¥ (sk-...)", type="password", value=st.session_state.OPENAI_API_KEY)
if st.sidebar.button("ğŸ’¾ í‚¤ ì €ì¥"):
    if api_key_input.startswith("sk-"):
        st.session_state.OPENAI_API_KEY = api_key_input
        st.sidebar.success("âœ… API í‚¤ ì €ì¥ ì™„ë£Œ")
    else:
        st.sidebar.warning("âš ï¸ ìœ íš¨í•œ OpenAI í‚¤ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")

# ======================================================
# API í‚¤ í™•ì¸ í›„ ì§„í–‰
# ======================================================
if not st.session_state.get("OPENAI_API_KEY", "").startswith("sk-"):
    st.warning("ğŸ‘ˆ ì™¼ìª½ì—ì„œ OpenAI API í‚¤ë¥¼ ë¨¼ì € ì…ë ¥í•˜ê³  ì €ì¥í•˜ì„¸ìš”.")
    st.stop()


# ======================================================
# 1. ë¶„ì„ í™˜ê²½ ì´ˆê¸°í™” (AnalysisInitializer) - í´ë” ìˆœíšŒ ë¡œì§ ìœ ì§€/ì‘ìš©
# ======================================================
class AnalysisInitializer:
    def __init__(self, uploaded_file):
        self._model = LLM_MODEL
        self._instruction = CUSTOM_INSTRUCTION
        self.uploaded_files = uploaded_files   # âœ… ì—¬ëŸ¬ íŒŒì¼ ì§€ì›
        self.llm: Optional[OpenAI] = None
        self.sdf: Optional[SmartDataframe] = None

    def initialize(self) -> Tuple[SmartDataframe, pd.DataFrame, OpenAI]:
        api_key = st.session_state["OPENAI_API_KEY"]
        openai.api_key = api_key

        df = self._load_data()

        # ğŸ‘ˆ LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì—¬ê¸°ì„œ ìƒì„±
        self.llm = OpenAI(api_token=api_key, model=self._model)

        # â˜… PandasAI v2.3.2 : df ê·¸ëŒ€ë¡œ ì „ë‹¬
        self.sdf = SmartDataframe(
            df,
            config={
                "llm": self.llm,
                "verbose": True,
                "memory": False,
                "instructions": CUSTOM_INSTRUCTION
            }
        )

        return self.sdf, df, self.llm

    # ======================================================
    # ì—‘ì…€ ë¡œë“œ â†’ ì „ì²˜ë¦¬ â†’ ì—¬ëŸ¬ ê°œ ì—…ë¡œë“œëœ íŒŒì¼ ë³‘í•©
    # ======================================================
    def _load_data(self) -> pd.DataFrame:
        if not self.uploaded_files:
            raise FileNotFoundError("âš ï¸ ì—…ë¡œë“œëœ ì—‘ì…€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

        excel_files = self.uploaded_files  # âœ… ì—¬ëŸ¬ íŒŒì¼ ì§ì ‘ ì‚¬ìš©

        print(f"ğŸ“‚ ì´ {len(excel_files)}ê°œ íŒŒì¼ ê°ì§€ë¨:")
        for f in excel_files:
            print(f" - {getattr(f, 'name', 'uploaded_file')}")

        all_dfs = []
        
        # --------------------------------------------------
        # 1ï¸âƒ£ ê°œë³„ íŒŒì¼ ì „ì²˜ë¦¬ (ì›ë³¸ê³¼ ë™ì¼í•œ ë¡œì§)
        # --------------------------------------------------
        for file in excel_files:
            file_name = getattr(file, "name", "uploaded_file")
            print(f"ğŸ”„ ì „ì²˜ë¦¬ ì¤‘: {file_name}")
            try:
                df_raw = pd.read_excel(file, header=None)

                # ìƒë‹¨ 4ì¤„ ì‚­ì œ
                df_raw = df_raw.iloc[4:].reset_index(drop=True)

                # ë¶ˆí•„ìš”í•œ ì—´ ì œê±° (E, G, I, K, L, N, O)
                drop_cols = [4, 6, 8, 10, 11, 13, 14]
                df_raw = df_raw.drop(df_raw.columns[drop_cols], axis=1)

                # ìƒˆ í—¤ë” ì§€ì •
                new_columns = [
                    "ì¥ë¹„ëª…", "UT", "Floor",
                    "ì‚¬ì „ì œì‘X_ë¹„ëŒ€ìƒ(ì¼ë¶€ê³µì •)(1)_ê¸¸ì´",
                    "ì‚¬ì „ì œì‘X_A(ì¥ë¹„ë‹¨Final)(2)_ê¸¸ì´",
                    "ì‚¬ì „ì œì‘â—‹_B(H_UPêµ¬ê°„)(3)_ë‹¹ì´ˆê³„íš_ê¸¸ì´",
                    "ì‚¬ì „ì œì‘â—‹_B(H_UPêµ¬ê°„)(4)_ì‹¤ì œì‹œê³µ_ê¸¸ì´",
                    "ì‚¬ì „ì œì‘X_C(TVë‹¨Final)(5)_ê¸¸ì´"
                ]
                df_raw.columns = new_columns

                # ìˆ«ìí˜• ë³€í™˜
                for col in new_columns[3:]:
                    df_raw[col] = pd.to_numeric(df_raw[col], errors="coerce")

                # í•©ê³„(1+2+4+5) ê³„ì‚°
                df_raw["í•©ê³„(1+2+4+5)"] = pd.to_numeric(
                    df_raw["ì‚¬ì „ì œì‘X_ë¹„ëŒ€ìƒ(ì¼ë¶€ê³µì •)(1)_ê¸¸ì´"].fillna(0)
                    + df_raw["ì‚¬ì „ì œì‘X_A(ì¥ë¹„ë‹¨Final)(2)_ê¸¸ì´"].fillna(0)
                    + df_raw["ì‚¬ì „ì œì‘â—‹_B(H_UPêµ¬ê°„)(4)_ì‹¤ì œì‹œê³µ_ê¸¸ì´"].fillna(0)
                    + df_raw["ì‚¬ì „ì œì‘X_C(TVë‹¨Final)(5)_ê¸¸ì´"].fillna(0),
                    errors="coerce"
                ).astype("float64")

                all_dfs.append(df_raw)
                print(f"âœ… {file_name} ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df_raw)}í–‰")

            except Exception as e:
                print(f"âŒ {file_name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        # --------------------------------------------------
        # 2ï¸âƒ£ ë³‘í•© (ì›ë³¸ êµ¬ì¡° ìœ ì§€)
        # --------------------------------------------------
        if not all_dfs:
            raise RuntimeError("âŒ ì „ì²˜ë¦¬ì— ì„±ê³µí•œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

        for df_raw in all_dfs:
            if "í•©ê³„(1+2+4+5)" in df_raw.columns:
                df_raw["í•©ê³„(1+2+4+5)"] = pd.to_numeric(
                    df_raw["í•©ê³„(1+2+4+5)"], errors="coerce"
                )

        merged_df = pd.concat(all_dfs, ignore_index=True)
        print(f"\nğŸ“Š ì „ì²´ ë³‘í•© ì™„ë£Œ: ì´ {len(merged_df)}í–‰, {len(merged_df.columns)}ì—´")

        return merged_df


# ======================================================
# 2. ì§ˆë¬¸ ê°€ê³µ ë¡œì§ (ìµœì¢… ì•ˆì • ë²„ì „ ê·¸ëŒ€ë¡œ)
# ======================================================
class PromptPreprocessor:
    def __init__(self):
        self._column_synonyms = COLUMN_SYNONYMS
        self._value_synonyms = VALUE_SYNONYMS
        self._ut_exclude = ["ì¥ë¹„", "ì¥ë¹„ë“¤"]

        self._dimension_ut_words = ["UT", "ê³µì¢…", "ì„¤ë¹„", "ìœ í‹¸ë¦¬í‹°", "utility"]
        self._dimension_device_words = ["ì¥ë¹„ëª…", "ì¥ë¹„"]
        self._dimension_floor_words = ["ì¸µ", "ì¸µìˆ˜"]

        # âœ… í•œê¸€ ì¡°ì‚¬ ë¦¬ìŠ¤íŠ¸ (í•„ìš”í•˜ë©´ ë” ì¶”ê°€í•´ë„ ë¨)
        self._josa_list = [
            "ì€", "ëŠ”", "ì´", "ê°€",
            "ì„", "ë¥¼", "ì˜",
            "ì—", "ì—ì„œ",
            "ë¡œ", "ìœ¼ë¡œ",
            "ì™€", "ê³¼",
            "ë„"
        ]

    def _normalize_column_words(self, prompt: str) -> str:
        """ì»¬ëŸ¼ ë™ì˜ì–´ + ë’¤ì— ë¶™ì€ ì¡°ì‚¬ê¹Œì§€ ì¸ì‹í•´ì„œ ì»¬ëŸ¼ëª…ì„ ì •ê·œí™”"""

        # ì¡°ì‚¬ íŒ¨í„´: ìœ„ ë¦¬ìŠ¤íŠ¸ ì¤‘ 1ê°œ ë˜ëŠ” 2ê¸€ìì§œë¦¬ ì¡°ì‚¬ë„ ìˆìœ¼ë‹ˆ ì „ì²´ OR
        josa_pattern = "(?:" + "|".join(self._josa_list) + ")?"

        for target, syns in self._column_synonyms.items():
            # syns(ë³„ì¹­) + target(ì •ê·œ ì»¬ëŸ¼ëª…) ë‘˜ ë‹¤ ì¡ë„ë¡
            for syn in syns + [target]:
                # âœ… í•œê¸€ì´ ë“¤ì–´ê°„ ë™ì˜ì–´ì¸ ê²½ìš°: ìš°ë¦¬ê°€ ì§ì ‘ ê²½ê³„ ì •ì˜ + ì¡°ì‚¬ í—ˆìš©
                if re.search(r"[ê°€-í£]", syn):
                    pattern = rf"(?<![ê°€-í£A-Za-z0-9])" \
                              rf"({re.escape(syn)})" \
                              rf"{josa_pattern}" \
                              rf"(?=[^ê°€-í£A-Za-z0-9]|$)"
                    prompt = re.sub(pattern, target, prompt)
                else:
                    # âœ… ì˜ë¬¸/ìˆ«ì ìœ„ì£¼ì˜ ë™ì˜ì–´(utility ë“±)ëŠ” ê¸°ì¡´ \b ë¡œ ê·¸ëŒ€ë¡œ ì²˜ë¦¬
                    pattern = rf"\b{re.escape(syn)}\b"
                    prompt = re.sub(pattern, target, prompt, flags=re.IGNORECASE)

        return prompt

    # ======================================================
    # ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜ (ì›ë³¸ ë¡œì§ ê·¸ëŒ€ë¡œ)
    # ======================================================
    def process(self, raw_prompt: str) -> str:
        if not raw_prompt:
            return ""

        prompt = raw_prompt.strip()
        conditions = []
        selected_columns = []
        dimension_columns = []

        # --------------------------------------------
        # 1. ê¸°ì¡´ ì»¬ëŸ¼/ê°’ ë™ì˜ì–´ ì¹˜í™˜
        # --------------------------------------------

        # ğŸš€ 'ì¥ë¹„'ë¥¼ 'equipment'ë¡œ ì¹˜í™˜í•˜ì—¬ ì»¬ëŸ¼ ë™ì˜ì–´ ì¶©ëŒ ë°©ì§€
        prompt = re.sub(r"\bì¥ë¹„\b", "equipment", prompt)

        # âœ… ì»¬ëŸ¼ëª…/ë³„ì¹­ + ì¡°ì‚¬ê¹Œì§€ í¬í•¨í•´ì„œ ì •ê·œí™”
        prompt = self._normalize_column_words(prompt)

        for target, syns in self._value_synonyms.items():
            for syn in syns:
                prompt = re.sub(
                    rf"\b{re.escape(syn)}\b", target, prompt, flags=re.IGNORECASE
                )

        prompt = re.sub(r"\bë°°ê´€\b", "ìœ í‹¸ë¦¬í‹°", prompt)
        prompt = re.sub(r"\bë¬¼ëŸ‰\b", "ë¬¼ëŸ‰ë“¤", prompt)

        # ----------------------------------------------------
        # 2. â­ ì°¨ì› ë¶„ì„(ë³„, ë„ì–´ì“°ê¸° ëª¨ë‘ ê°ì§€)
        # ----------------------------------------------------

        for word in self._dimension_ut_words:
            if re.search(rf"{word}\s*ë³„", raw_prompt, flags=re.IGNORECASE):
                dimension_columns.append("UT")
                prompt = re.sub(rf"{word}\s*ë³„", "", prompt)
                break

        for word in self._dimension_device_words:
            if re.search(rf"{word}\s*ë³„", raw_prompt, flags=re.IGNORECASE):
                dimension_columns.append("ì¥ë¹„ëª…")
                prompt = re.sub(rf"{word}\s*ë³„", "", prompt)
                break

        for word in self._dimension_floor_words:
            if re.search(rf"{word}\s*ë³„", raw_prompt, flags=re.IGNORECASE):
                dimension_columns.append("Floor")
                prompt = re.sub(rf"{word}\s*ë³„", "", prompt)
                break

        # ----------------------------------------------------
        # 3. ê¸°ë³¸ ì¡°ê±´(Floor/UT/ì¥ë¹„ëª… ê°ì§€)
        # ----------------------------------------------------

        for fl in ["1F", "2F", "3F"]:
            if re.search(rf"\b{fl}\b", prompt):
                conditions.append(f'(Floor == "{fl}")')
                prompt = re.sub(rf"\b{fl}\b", "", prompt)

        for val in self._value_synonyms.keys():
            if val not in ["1F", "2F", "3F"] and val not in self._ut_exclude:
                if re.search(rf"\b{val}\b", prompt):
                    conditions.append(f'(UT == "{val}")')
                    prompt = re.sub(rf"\b{val}\b", "", prompt)

        device_matches = [
            word for word in re.findall(r"\b[A-Za-z0-9]{3,}\b", prompt)
            if any(c.isalpha() for c in word) and any(c.isdigit() for c in word)
        ]

        for dev in device_matches:
            conditions.append(f'(ì¥ë¹„ëª… == "{dev}")')
            prompt = prompt.replace(dev, "")

        # ----------------------------------------------------
        # 4. ì¶œë ¥ ì»¬ëŸ¼ ìë™ ê°ì§€
        # ----------------------------------------------------

        for col in self._column_synonyms.keys():
            if col in prompt:
                selected_columns.append(col)
                prompt = prompt.replace(col, "")

        # ----------------------------------------------------
        # 5. ëª…ë ¹ì–´ í‘œì¤€í™” (ë‹¨ìˆœí™” ë¡œì§ ì ìš©)
        # ----------------------------------------------------

        # ë„ì–´ì“°ê¸°ë¥¼ ì œì™¸í•œ ë¬¸ìì—´ì—ì„œ í•œê¸€ë§Œ ì¶”ì¶œ
        korean_chars = re.sub(r"[^ê°€-í£]", "", prompt)

        # í•œê¸€ì´ 2ê¸€ì ì´ìƒ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ í‘œì¤€ ëª…ë ¹ ì‚½ì…
        if len(korean_chars) >= 2:

            # ê¸°ì¡´ì— ìˆë˜ 'ë³´ì—¬ì¤˜/ì•Œë ¤ì¤˜/êµ¬í•´ì¤˜' ë“±ì˜ íŒ¨í„´ì„ ë¨¼ì € ì œê±°í•©ë‹ˆë‹¤.
            command_patterns = r"(ë³´ì—¬ì¤˜|ì•Œë ¤ì¤˜|êµ¬í•´ì¤˜|ë¦¬ìŠ¤íŠ¸í•´ì¤˜|ì •ë¦¬í•´ì¤˜|ëª©ë¡í™”í•´ì¤˜|í•©ì€|ì´í•©ì€|í•©ê³„ëŠ”|ì´ëŸ‰ì€|ëª‡ì´ì•¼|ëª‡ê°œì•¼|ì–¼ë§ˆì•¼|ì–´ë–»ê²Œ ë¼|ì–¼ë§ˆì¸ì§€|ê²°ê³¼ëŠ”)"
            prompt = re.sub(command_patterns, "", prompt)

            # ìƒˆë¡œìš´ í‘œì¤€ ëª…ë ¹ ì‚½ì…
            prompt = prompt.strip() + " ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³´ì—¬ì¤˜"

        # ----------------------------------------------------
        # 6. ì¡°ë¦½
        # ----------------------------------------------------

        final_parts = []
        if conditions:
            final_parts.append(" AND ".join(conditions))
        if selected_columns:
            final_parts.append(f"ì¶œë ¥ì»¬ëŸ¼ = {['ì¥ë¹„ëª…','UT','Floor'] + selected_columns}")
        if dimension_columns:
            final_parts.append(f"ì°¨ì›ì»¬ëŸ¼ = {dimension_columns}")
            final_parts.append("ì§‘ê³„ë°©ì‹ = 'sum'")
        final_parts.append(prompt)
        final = " ".join(final_parts)
        final = re.sub(r"\s+", " ", final).strip()

        return final


# ======================================================
# ìŠ¤ë§ˆíŠ¸ ì‘ë‹µ ëª¨ë“ˆ (Smart Response Engine) - ì›ë³¸ ê·¸ëŒ€ë¡œ
# ======================================================
class SmartResponseEngine:
    def __init__(self):
        pass

    # 1. ê²°ê³¼ê°€ DFì¸ì§€ í™•ì¸
    def is_dataframe(self, result: Any) -> bool:
        if isinstance(result, dict) and result.get("type") == "dataframe":
            return True
        if isinstance(result, pd.DataFrame):
            return True
        return False

    # 2. DF ë¶„ì„ (SUM/MEAN/MAX/MIN ê³„ì‚°)
    def analyze_dataframe(self, df: pd.DataFrame) -> Dict[str, Dict[str, Any]]:
        df = df.apply(pd.to_numeric, errors="ignore")
        numeric_cols = df.select_dtypes(include=["int64", "float64"]).columns
        stats = {}

        for col in numeric_cols:
            stats[col] = {
                "sum": float(df[col].sum()),
                "mean": float(df[col].mean()),
                "max": float(df[col].max()),
                "min": float(df[col].min())
            }

        return stats

    # 3. ìŠ¤ë§ˆíŠ¸ ì‘ë‹µì„ DataFrame í˜•íƒœë¡œ ìƒì„± (llm ì¸ìŠ¤í„´ìŠ¤ ì¶”ê°€)
    def generate_smart_response(self, df_stats: Dict, prompt: str, llm_instance: OpenAI) -> Tuple[str, pd.DataFrame]:
        # â–¶ï¸ 1. í†µê³„í‘œ ìƒì„±ìš© ë°ì´í„°
        stats_dict = {
            "SUM": {},
            "MEAN": {},
            "MAX": {},
            "MIN": {}
        }

        def format_value(v: float) -> str:
            rounded = round(v, 2)
            return str(int(rounded)) if rounded == int(rounded) else f"{rounded:.2f}"

        # âœ… í•©ê³„(1+2+4+5) ì»¬ëŸ¼ì˜ sumì„ ê¸°ì¤€ total_sumìœ¼ë¡œ ì‚¬ìš©
        total_sum_entry = df_stats.get("í•©ê³„(1+2+4+5)")
        total_sum = float(total_sum_entry["sum"]) if total_sum_entry else 0.0

        # â–¶ï¸ 2. JSON êµ¬ì¡° ìƒì„± (LLM ì „ë‹¬ìš©)
        stats_json = {}
        for col, values in df_stats.items():
            entry = {"sum": round(values["sum"], 2)}

            # âœ… total_sum ë¹„ìœ¨ ê³„ì‚°
            if total_sum > 0:
                entry["ratio_to_total"] = round(values["sum"] / total_sum * 100, 1)

            # âœ… ê³„íš ëŒ€ë¹„ ì‹œê³µ ë¹„ìœ¨ ê³„ì‚°
            if "ë‹¹ì´ˆê³„íš" in col:
                for other_col in df_stats:
                    if "ì‹¤ì œì‹œê³µ" in other_col:
                        plan = values["sum"]
                        real = df_stats[other_col]["sum"]
                        if plan > 0:
                            ratio = round(real / plan * 100, 1)
                            diff = round(real - plan, 2)
                            trend = (
                                "ì‹œê³µì´ ê³„íšë³´ë‹¤ ë§ìŒ" if real > plan
                                else "ì‹œê³µì´ ê³„íšë³´ë‹¤ ì ìŒ" if real < plan
                                else "ê³„íšê³¼ ë™ì¼"
                            )
                            stats_json[other_col] = stats_json.get(other_col, {})
                            stats_json[other_col].update({
                                "plan_to_real_ratio": ratio,
                                "plan_to_real_diff": diff,
                                "plan_to_real_trend": trend
                            })

            stats_json[col] = entry

            # âœ… í•©ê³„ ì»¬ëŸ¼ì€ SUMë§Œ í‘œì‹œ, ë‚˜ë¨¸ì§€ëŠ” '-'
            if col == "í•©ê³„(1+2+4+5)":
                stats_dict["SUM"][col] = format_value(values["sum"])
                stats_dict["MEAN"][col] = "-"
                stats_dict["MAX"][col] = "-"
                stats_dict["MIN"][col] = "-"
            else:
                stats_dict["SUM"][col] = format_value(values["sum"])
                stats_dict["MEAN"][col] = format_value(values["mean"])
                stats_dict["MAX"][col] = format_value(values["max"])
                stats_dict["MIN"][col] = format_value(values["min"])

        # âœ… ì‚¬ì „ì œì‘ ë¬¼ëŸ‰ vs ë¹„ì§„í–‰ ë¬¼ëŸ‰ ë¹„êµ 
        try:
            pre_fab_sum = df_stats.get("ì‚¬ì „ì œì‘â—‹_B(H_UPêµ¬ê°„)(4)_ì‹¤ì œì‹œê³µ_ê¸¸ì´", {}).get("sum", 0)
            non_pre_fab_sum = sum([
                df_stats.get("ì‚¬ì „ì œì‘X_ë¹„ëŒ€ìƒ(ì¼ë¶€ê³µì •)(1)_ê¸¸ì´", {}).get("sum", 0),
                df_stats.get("ì‚¬ì „ì œì‘X_A(ì¥ë¹„ë‹¨Final)(2)_ê¸¸ì´", {}).get("sum", 0),
                df_stats.get("ì‚¬ì „ì œì‘X_C(TVë‹¨Final)(5)_ê¸¸ì´", {}).get("sum", 0)
            ])

            if non_pre_fab_sum > 0:
                pre_ratio = round(pre_fab_sum / non_pre_fab_sum * 100, 1)

                if pre_ratio > 100:
                    trend = f"ì‚¬ì „ì œì‘ ì§„í–‰ë¬¼ëŸ‰ì€ ë¹„ì§„í–‰ ë¬¼ëŸ‰ë³´ë‹¤ {round(pre_ratio / 100, 2)}ë°° ë§ìŠµë‹ˆë‹¤."
                elif pre_ratio == 100:
                    trend = "ì‚¬ì „ì œì‘ ì§„í–‰ë¬¼ëŸ‰ê³¼ ë¹„ì§„í–‰ ë¬¼ëŸ‰ì€ ë™ì¼í•œ ìˆ˜ì¤€ì…ë‹ˆë‹¤."
                else:
                    trend = f"ì‚¬ì „ì œì‘ ì§„í–‰ë¬¼ëŸ‰ì€ ë¹„ì§„í–‰ ë¬¼ëŸ‰ ëŒ€ë¹„ {pre_ratio}% ìˆ˜ì¤€ìœ¼ë¡œ ìƒëŒ€ì ìœ¼ë¡œ ì ìŠµë‹ˆë‹¤."

                stats_json["ì‚¬ì „ì œì‘_ë¹„ì§„í–‰_ë¹„êµ"] = {
                    "ì‚¬ì „ì œì‘_ë¬¼ëŸ‰í•©ê³„": round(pre_fab_sum, 2),
                    "ë¹„ì§„í–‰_ë¬¼ëŸ‰í•©ê³„": round(non_pre_fab_sum, 2),
                    "ì‚¬ì „ì œì‘_ë¹„ìœ¨(ë¹„ì§„í–‰_ê¸°ì¤€%)": pre_ratio,
                    "ë¹„êµê²°ê³¼": trend
                }

        except Exception as e:
            stats_json["ì‚¬ì „ì œì‘_ë¹„ì§„í–‰_ë¹„êµ"] = {"ì˜¤ë¥˜": str(e)}

        stats_df = pd.DataFrame(stats_dict)

        stats_json_str = json.dumps(stats_json, ensure_ascii=False, indent=2)

        # â–¶ï¸ 3. LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        insight_prompt = f"""
ë‹¤ìŒì€ íŠ¹ì • ì„¤ë¹„ ë°°ê´€ ë°ì´í„°ì— ëŒ€í•œ ì •ëŸ‰ ë¶„ì„ ê²°ê³¼ì´ë‹¤.
ì£¼ì–´ì§„ ìˆ˜ì¹˜ë¥¼ ì°¸ê³ í•˜ì—¬ í˜„ì¥ ì—”ì§€ë‹ˆì–´ ê´€ì ì—ì„œ ì˜ë¯¸ ìˆëŠ” ì¸ì‚¬ì´íŠ¸ë¥¼ 5ë¬¸ì¥ ì´ë‚´ë¡œ ìƒì„±í•˜ë¼.

ë‹¨:
- ì–´ë–¤ í•­ëª©ì„ ê°•ì¡°í• ì§€ ìŠ¤ìŠ¤ë¡œ íŒë‹¨í•˜ë¼.
- ë¹„ìœ¨ ë° ë³€í™”ëŸ‰ì€ ë°˜ë“œì‹œ JSONì— ì œê³µëœ ìˆ«ìë§Œ ì‚¬ìš©í•œë‹¤.
- 'ê³„íš ëŒ€ë¹„ ì‹œê³µ', 'í•©ê³„ ëŒ€ë¹„ ë¹„ìœ¨', 'ê°€ì¥ í° í•­ëª©', 'ì‚¬ì „ì œì‘ ì§„í–‰ë¬¼ëŸ‰' ë“±ì€ í•„ìš” ì‹œ ì„ íƒì ìœ¼ë¡œ ì–¸ê¸‰í•˜ë¼.
- 'ì‚¬ì „ì œì‘ vs ë¹„ì§„í–‰ ë¹„êµ'ëŠ” ë°˜ë“œì‹œ "ì‚¬ì „ì œì‘ì€ ë¹„ì§„í–‰ ëŒ€ë¹„ xx% ìˆ˜ì¤€"ìœ¼ë¡œ í‘œí˜„í•˜ë¼.
- ì‚¬ì „ì œì‘ì´ ì ì€ ê²½ìš° 'ì‘ê²Œ ë‚˜íƒ€ë‚¬ë‹¤' ë˜ëŠ” 'ìƒëŒ€ì ìœ¼ë¡œ ì ë‹¤' ë“±ì˜ í‘œí˜„ì„ ì‚¬ìš©í•  ê²ƒ.
- ì¸ì‚¬ì´íŠ¸ëŠ” 'ë°ì´í„°ë¥¼ í•´ì„í•œ ë¬¸ì¥'ì´ì–´ì•¼ í•˜ë©°, ë‹¤ì‹œ ìˆ«ìë¥¼ ë‚˜ì—´í•˜ì§€ ë§ˆë¼.

JSON ë°ì´í„°:
{{
  "stats": {stats_json_str},
  "total_sum": {round(total_sum, 2)}
}}
        """.strip()

        # â–¶ï¸ 4. LLM í˜¸ì¶œ (ì§ì ‘ OpenAI SDK ì‚¬ìš©)
        try:
            from openai import OpenAI as OpenAIClient
            client = OpenAIClient(api_key=openai.api_key)

            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "You are a senior data analysis assistant."},
                    {"role": "user", "content": insight_prompt}
                ],
                temperature=0.7,
                max_tokens=400
            )

            insight = response.choices[0].message.content.strip()

        except Exception as e:
            insight = f"âš ï¸ ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}"

        # â–¶ï¸ 5. ì§ˆë¬¸ ìš”ì•½ + ì¸ì‚¬ì´íŠ¸ ì¡°í•©
        cleaned_prompt = self.clean_prompt_for_summary(prompt)
        summary_text = (
            "ğŸ“Œ **AI ìŠ¤ë§ˆíŠ¸ ë¶„ì„ ê²°ê³¼**\n\n"
            f"ğŸ’¬ ë¶„ì„ ìš”ì²­ ìš”ì•½: **{cleaned_prompt}**\n\n"
            f"ğŸ§  **LLM ì¸ì‚¬ì´íŠ¸ ìš”ì•½:**\n\n{insight.strip()}\n"
        )

        return summary_text, stats_df

    # 4. ìŠ¤ë§ˆíŠ¸ ìì—°ì–´ ì‘ë‹µì—ì„œ ë°ì´í„° í”„ë ˆì„ ë¬¸êµ¬ ì œê±°
    def clean_prompt_for_summary(self, prompt: str) -> str:
        """
        ìì—°ì–´ ì‘ë‹µìš© ì§ˆë¬¸ ì •ì œ: 
        'ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³´ì—¬ì¤˜' â†’ 'ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ AI ë¶„ì„ì„ í†µí•´ ì¸ì‚¬ì´íŠ¸ì™€ í™œìš©ë°©ì•ˆì„ ì œê³µí•©ë‹ˆë‹¤.'
        """
        # ì œê±°í•  ëª…ë ¹ì–´ íŒ¨í„´
        remove_patterns = [
            r"ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ\s*ë³´ì—¬ì¤˜",
            r"ë³´ì—¬ì¤˜",
            r"ë°ì´í„°í”„ë ˆì„",
            r"ì•Œë ¤ì¤˜",
            r"êµ¬í•´ì¤˜",
            r"ë¦¬ìŠ¤íŠ¸í•´ì¤˜",
            r"ì •ë¦¬í•´ì¤˜",
            r"ëª©ë¡í™”í•´ì¤˜",
            r"ê²°ê³¼ëŠ”",
            r"í•©ì€",
            r"ì´í•©ì€"
        ]

        clean = prompt

        # ë¶ˆí•„ìš” ëª…ë ¹ ì œê±°
        for p in remove_patterns:
            clean = re.sub(p, "", clean).strip()

        # ë§ˆì§€ë§‰ ë¬¸êµ¬ë¥¼ ê³ ì •í•´ì„œ ë¶™ì—¬ì¤Œ
        clean += " â€” ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ AI ë¶„ì„ì„ í†µí•´ ì¸ì‚¬ì´íŠ¸ì™€ í™œìš©ë°©ì•ˆì„ ì œê³µí•©ë‹ˆë‹¤."

        return clean


# ======================================================
# 4. Streamlit UI (CTk App.perform_analysis ë¡œì§ì„ ê·¸ëŒ€ë¡œ ì˜®ê¹€)
# ======================================================

st.title("ğŸ“Š PandasAI ëŒ€í™”í˜• ë°ì´í„° ë¶„ì„ê¸°")
st.markdown("---")

# --- ì‚¬ì´ë“œë°”: ì—‘ì…€ ì—…ë¡œë“œ ---
st.sidebar.header("ğŸ“ ì—‘ì…€ ì—…ë¡œë“œ")
uploaded_files = st.sidebar.file_uploader(
    "ì‚¬ì „ë°°ê´€ì œì‘ ë¬¼ëŸ‰ ì—‘ì…€ íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš” (.xlsx)",
    type=["xlsx"],
    accept_multiple_files=True  # âœ… ì—¬ëŸ¬ ê°œ íŒŒì¼ í—ˆìš©
)

if not uploaded_files:
    st.info("ğŸ‘ˆ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ë¶„ì„ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    st.stop()
    
initializer = AnalysisInitializer(uploaded_files)  # ë¦¬ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ì „ë‹¬

# --- ì´ˆê¸°í™” (RESET_ON_QUERY ê³ ë ¤í•´ì„œ ì„¸ì…˜ì— ì €ì¥) ---
if "sdf" not in st.session_state or "df" not in st.session_state or "llm" not in st.session_state or RESET_ON_QUERY:
    sdf_instance, df, llm_instance = initializer.initialize()
    st.session_state.sdf = sdf_instance
    st.session_state.df = df
    st.session_state.llm = llm_instance
else:
    sdf_instance = st.session_state.sdf
    df = st.session_state.df
    llm_instance = st.session_state.llm

preprocessor = PromptPreprocessor()
engine = SmartResponseEngine()

st.markdown("## ğŸ’¬ ë¶„ì„ ì§ˆë¬¸ ì…ë ¥")

with st.form("query_form"):
    user_query = st.text_input(
        "ë¶„ì„í•  ë‚´ìš©ì„ ì…ë ¥í•˜ê³  Enter ë˜ëŠ” ë²„íŠ¼ì„ ëˆŒëŸ¬ ì‹¤í–‰í•˜ì„¸ìš”.",
        placeholder="ì˜ˆ: 5TFSP1001 2ì¸µ í†¡ì‹ê°€ìŠ¤ ë¬¼ëŸ‰ ì•Œë ¤ì¤˜"
    )
    submitted = st.form_submit_button("ğŸš€ AI ë¶„ì„ ì‹¤í–‰")

if submitted:
    if not user_query.strip():
        st.warning("ë¶„ì„ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        with st.spinner("â³ AIê°€ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
            # 1) ì§ˆë¬¸ ê°€ê³µ
            processed = preprocessor.process(user_query)

            # 2) PandasAI ì‹¤í–‰ (CTkì˜ perform_analysis ë¡œì§ ëŒ€ì‘)
            try:
                result = sdf_instance.chat(processed)
                generated_code = sdf_instance.last_code_generated
            except Exception as e:
                st.error(f"âŒ ë¶„ì„ ì˜¤ë¥˜: {e}")
                st.stop()

            # 3) ìƒë‹¨: ì§ˆë¬¸ ê°€ê³µ ê²°ê³¼
            st.markdown("### âœ¨ ì§ˆë¬¸ ê°€ê³µ ê²°ê³¼")
            st.code(processed, language="text")

            # 4) ì¤‘ê°„: LLM ìƒì„± ì½”ë“œ
            st.markdown("### ğŸ’» LLM ìƒì„± ì½”ë“œ")
            st.code(generated_code, language="python")

            # 5) í•˜ë‹¨: AI ë¶„ì„ ê²°ê³¼ + ìŠ¤ë§ˆíŠ¸ í†µê³„ ìš”ì•½
            st.markdown("### ğŸ’¡ AI ë¶„ì„ ê²°ê³¼")

            if engine.is_dataframe(result):
                df_out = result.get("value", result)

                # âœ… ì´ í•œ ì¤„ë¡œ ì‹¤ì œ í•„í„°ë§ëœ dfë¥¼ í™”ë©´ì— í‘œì‹œ
                st.subheader("ğŸ“‹ í•„í„°ë§ëœ ë°ì´í„°í”„ë ˆì„ ê²°ê³¼")
                st.dataframe(df_out)
                
                # í†µê³„ ë¶„ì„
                stats = engine.analyze_dataframe(df_out)

                # ìŠ¤ë§ˆíŠ¸ ì‘ë‹µ ìƒì„±
                summary_text, smart_df = engine.generate_smart_response(
                    stats, processed, llm_instance
                )

                # ìš”ì•½ í…ìŠ¤íŠ¸
                st.markdown(summary_text)

                # í†µê³„ DF ì¶œë ¥
                st.markdown("#### ğŸ“Š [AI ìŠ¤ë§ˆíŠ¸ í†µê³„ ìš”ì•½]")
                st.dataframe(smart_df)
            else:
                # resultê°€ DFê°€ ì•„ë‹ˆë¼ë©´ ê·¸ëŒ€ë¡œ ì¶œë ¥
                st.write(result)











